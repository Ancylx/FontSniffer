import random
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Callable, Dict, Generator, List, Optional, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup


class FontSniffer:
    """æ™ºèƒ½å­—ä½“çˆ¬è™« - æ”¯æŒå¹¶å‘ã€é‡è¯•ã€åŠ¨æ€é…ç½®"""

    def __init__(self, user_agent: str, max_workers: int = 8) -> None:
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            user_agent: User-Agent å­—ç¬¦ä¸²
            max_workers: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®® 5-10ï¼‰
        """
        # è¯·æ±‚é…ç½®
        self.base_url = "http://www.downcc.com/font/list_200_{page}.html"
        self.timeout = 15
        self.max_retries = 3
        self.base_delay = 0.3

        # åˆ›å»º Session å¤ç”¨è¿æ¥
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Connection": "keep-alive",
        })

        # å¹¶å‘é…ç½®
        self.max_workers = max(1, min(max_workers, 20))

        # çŠ¶æ€æ§åˆ¶ï¼ˆå¯ç”± GUI é‡å†™ï¼‰
        self.should_stop: Callable[[], bool] = lambda: False

        # é¢„ç¼–è¯‘æ­£åˆ™
        self._page_regex = re.compile(r"list_200_(\d+)\.html")
        self._font_regex = re.compile(r"/font/")

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "retried_requests": 0,
        }

    def _detect_total_pages(self) -> int:
        """
        æ™ºèƒ½æ£€æµ‹æ€»é¡µæ•°
        
        Returns:
            æ€»é¡µæ•°ï¼Œå¤±è´¥æ—¶è¿”å›383
        """
        try:
            html = self._fetch_page(1)
            if not html:
                return 383

            soup = BeautifulSoup(html, "html.parser")
            pager = soup.find("div", class_="pages")

            if pager:
                page_links = pager.find_all("a", href=self._page_regex)
                if page_links:
                    page_numbers = [
                        int(match.group(1))
                        for link in page_links
                        if (match := self._page_regex.search(link.get("href", "")))
                    ]
                    return max(page_numbers) if page_numbers else 383

            return 383
        except Exception as e:
            print(f"é¡µæ•°æ£€æµ‹å¤±è´¥: {e}")
            return 383

    def _fetch_page(self, page: int, retry_count: int = 0) -> Optional[str]:
        """
        è·å–å•é¡µå†…å®¹ï¼ˆæ™ºèƒ½é‡è¯•ï¼‰

        Args:
            page: é¡µç 
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°

        Returns:
            HTML æ–‡æœ¬æˆ– None
        """
        if self.should_stop():
            return None

        try:
            self.stats["total_requests"] += 1

            url = self.base_url.format(page=page)
            response = self.session.get(url, timeout=self.timeout, allow_redirects=False)
            response.raise_for_status()
            response.encoding = "utf-8"

            self.stats["successful_requests"] += 1
            return response.text
        except requests.exceptions.RequestException as e:
            self.stats["failed_requests"] += 1

            if retry_count < self.max_retries and not self.should_stop():
                self.stats["retried_requests"] += 1

                # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                delay = self.base_delay * (2  ** retry_count) + random.uniform(0.1, 0.3)
                print(f"ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥: {str(e)[:50]}... | "
                      f"{delay:.2f}ç§’åé‡è¯•({retry_count+1}/{self.max_retries})")

                time.sleep(delay)
                return self._fetch_page(page, retry_count + 1)

            print(f"ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥ï¼Œå·²è·³è¿‡: {str(e)[:50]}...")
            return None

    def _parse_and_filter_page(self, page: int) -> Tuple[int, List[Tuple[str, str]]]:
        """
        è§£æå¹¶è¿‡æ»¤å•é¡µ

        Args:
            page: é¡µç 

        Returns:
            (é¡µç , åŒ¹é…ç»“æœåˆ—è¡¨)
        """
        html = self._fetch_page(page)
        if not html:
            return page, []

        soup = BeautifulSoup(html, "html.parser")
        target_section = soup.find("section", class_="mg-t10 border soft-list")
        if not target_section:
            return page, []

        font_ul = target_section.find("ul", id="li-change-color",
                                     class_="soft-list-bd hover-one")
        if not font_ul:
            return page, []

        results = []
        keyword_lower = self._keyword.lower() if hasattr(self, '_keyword') else ""

        for li in font_ul.find_all("li"):
            font_a_tag = li.find("a", class_="mg-r10",
                               href=lambda x: x and self._font_regex.match(x))
            if not font_a_tag:
                continue

            font_name = font_a_tag.get_text(strip=True)
            if keyword_lower and keyword_lower not in font_name.lower():
                continue

            relative_url = font_a_tag.get("href", "")
            full_url = urljoin("http://www.downcc.com", relative_url)
            results.append((font_name, full_url))

        return page, results

    def search(self, keyword: str) -> Generator[Dict[str, str], None, None]:
        """
        å¹¶å‘æœç´¢å­—ä½“

        Args:
            keyword: æœç´¢å…³é”®è¯

        Yields:
            çŠ¶æ€æˆ–ç»“æœå­—å…¸
        """
        self._keyword = keyword

        # åŠ¨æ€æ£€æµ‹é¡µæ•°
        yield {"type": "status", "content": "æ­£åœ¨æ£€æµ‹æ€»é¡µæ•°..."}
        total_pages = self._detect_total_pages()
        yield {"type": "status", "content": f"âœ… æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}"}

        # åˆå§‹åŒ–å¹¶å‘
        yield {"type": "status",
               "content": f"å¯åŠ¨ {self.max_workers} ä¸ªå¹¶å‘çº¿ç¨‹ | å…³é”®è¯: '{keyword}'"}

        completed_pages = 0
        total_found = 0

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_page = {
                executor.submit(self._parse_and_filter_page, page): page
                for page in range(1, total_pages + 1)
            }

            # æŒ‰å®Œæˆé¡ºåºå¤„ç†
            for future in as_completed(future_to_page):
                if self.should_stop():
                    executor.shutdown(wait=False)
                    yield {"type": "status", "content": "â¹ æœç´¢å·²ä¸­æ­¢"}
                    break

                page = future_to_page[future]
                try:
                    _, matched_fonts = future.result()
                    completed_pages += 1

                    # è¿›åº¦çŠ¶æ€
                    progress_msg = (
                        f"ç¬¬{page}é¡µå®Œæˆ | "
                        f"å·²å¤„ç† {completed_pages}/{total_pages} é¡µ | "
                        f"æ‰¾åˆ° {total_found} ä¸ªå­—ä½“"
                    )
                    yield {"type": "status", "content": progress_msg}

                    # å‘é€åŒ¹é…ç»“æœ
                    if matched_fonts:
                        total_found += len(matched_fonts)
                        for font_name, font_url in matched_fonts:
                            yield {
                                "type": "result",
                                "content": f'"{font_name}" ç¬¦åˆæ¡ä»¶\nä¸‹è½½é¡µé¢ï¼š{font_url}'
                            }
                except Exception as e:
                    yield {"type": "status",
                           "content": f"ç¬¬{page}é¡µå¤„ç†å¼‚å¸¸: {str(e)[:50]}..."}

        # å®ŒæˆæŠ¥å‘Š
        if not self.should_stop():
            yield {
                "type": "status",
                "content": (
                    f"\n{'='*60}\n"
                    f"âœ… æœç´¢å®Œæˆï¼å…±æ‰¾åˆ° {total_found} ä¸ªå­—ä½“\n"
                    f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡: æ€»={self.stats['total_requests']} "
                    f"| æˆåŠŸ={self.stats['successful_requests']} "
                    f"| å¤±è´¥={self.stats['failed_requests']} "
                    f"| é‡è¯•={self.stats['retried_requests']}"
                )
            }

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats.copy()


# ç›´æ¥è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    import time

    print("=" * 60)
    print("å­—ä½“å—…æ¢å™¨ - æ€§èƒ½æµ‹è¯•æ¨¡å¼")
    print("=" * 60)

    default_ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    sniffer = FontSniffer(user_agent=default_ua, max_workers=10)

    keyword = input("è¯·è¾“å…¥æµ‹è¯•å…³é”®è¯ (é»˜è®¤: å®‹ä½“): ").strip() or "å®‹ä½“"

    print(f"\nå¼€å§‹æœç´¢ '{keyword}'...")
    print(f"å¹¶å‘çº¿ç¨‹: {sniffer.max_workers}")
    print("-" * 60)

    start_time = time.time()
    found = 0

    try:
        for item in sniffer.search(keyword):
            if item["type"] == "result":
                found += 1
            elif item["type"] == "status" and ("å®Œæˆ" in item["content"]
                                             or "ä¸­æ­¢" in item["content"]):
                print(f"ã€çŠ¶æ€ã€‘{item['content']}")
    except KeyboardInterrupt:
        print("\nâ¹ ç”¨æˆ·ä¸­æ­¢æœç´¢")

    elapsed = time.time() - start_time

    print("-" * 60)
    print(f"ğŸ¯ æµ‹è¯•å®Œæˆï¼")
    print(f"â±ï¸  è€—æ—¶: {elapsed:.2f} ç§’")
    print(f"ğŸ“ æ‰¾åˆ°å­—ä½“: {found} ä¸ª")
    print(f"ğŸ”§ å¹¶å‘æ•°: {sniffer.max_workers}")
    print(f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡: {sniffer.get_stats()}")